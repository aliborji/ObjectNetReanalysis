{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "from overlapped_classes import overlapped_classes\n",
    "import json\n",
    "from PIL import Image\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../mappings/objectnet_to_imagenet_1k.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"../mappings/pytorch_to_imagenet_2012_id.json\") as f:\n",
    "    idxMap = json.load(f)\n",
    "        \n",
    "with open(\"../mappings/folder_to_objectnet_label.json\") as f:\n",
    "    folderMap = json.load(f)\n",
    "\n",
    "with open('imagenet_classes.txt') as f:\n",
    "# with open('../mappings/imagenet_to_label_2012_v2.txt') as f:    \n",
    "    labels = [line.strip() for line in f.readlines()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverting the folderMap\n",
    "dirMap = {}\n",
    "for u, v in folderMap.items():\n",
    "    dirMap[v]= u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(json.load)\n",
    "# for i in overlapped_classes:\n",
    "#     if not data[i] in labels:\n",
    "#         print(f\"{i} : {data[i]}, {data[i] in labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapped_classes\n",
    "# data['Alarm clock']\n",
    "# idxMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in overlapped_classes:\n",
    "#     print(data[i] in labels)\n",
    "#     print(f\"{i} : {data[i]}\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "dir(models)\n",
    "\n",
    "\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "# model = models.resnet152(pretrained=True)\n",
    "# model = models.inception_v3(pretrained=True)\n",
    "# model = models.googlenet(pretrained=True)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, box, draw=False):\n",
    "    # Define transformations for the image, should (note that imagenet models are trained with image size 224)\n",
    "    transform = transforms.Compose([\n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))    \n",
    "\n",
    "    ])\n",
    "    \n",
    "    \n",
    "#     transform = transforms.Compose([            #[1]\n",
    "#     #  transforms.Resize(256),                    #[2]\n",
    "#     #  transforms.CenterCrop(224),                #[3]\n",
    "#      transforms.ToTensor(),                     #[4]\n",
    "#     #  transforms.Normalize(                      #[5]\n",
    "#     #  mean=[0.485, 0.456, 0.406],                #[6]\n",
    "#     #  std=[0.229, 0.224, 0.225]                  #[7]\n",
    "#     #  )\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    \n",
    "#     ])\n",
    "    \n",
    "#     print(\"Prediction in progress\")\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    \n",
    "    xl, yl, xr, yr = box\n",
    "    img = img.crop((xl, yl, xr, yr))   #((left, top, right, bottom)) \n",
    "    img = img.resize((224, 224))\n",
    "\n",
    "    img_t = transform(img).float()\n",
    "\n",
    "#     img_t = img_t.permute((0,1,2))\n",
    "#     img_t = torch.transpose(img_t, 0, 1)  # rotate\n",
    "    \n",
    "    if draw:\n",
    "        plt.imshow(img) #img_t.permute((2,1,0)) )\n",
    "    # ##    plt.imshow(torch.transpose(img_t.permute((1,2,0)),  0, 1))\n",
    "        plt.show()\n",
    "\n",
    "#     print(img_t.shape)\n",
    "    # Preprocess the image\n",
    "#     image_tensor = transformation(image).float()\n",
    "\n",
    "    # Add an extra batch dimension since pytorch treats all images as batches\n",
    "#     image_tensor = image_tensor.unsqueeze_(0)\n",
    "    image_tensor = img_t.unsqueeze_(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "#     input = Variable(image_tensor)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    output = model(image_tensor)\n",
    "\n",
    "#     index = output.data.numpy().argmax()  # top 1\n",
    "    _, indices = torch.sort(output.data, descending=True)\n",
    "#     [(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]\n",
    "    return indices[0][:5] # index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alarm_clock'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "dirMap[category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> Alarm clock: top 1: 6.25  -  top 5: 20.13888888888889      [num imgs: 144]\n",
      "1 -> Backpack: top 1: 27.659574468085108  -  top 5: 53.61702127659574      [num imgs: 235]\n",
      "2 -> Banana: top 1: 44.255319148936174  -  top 5: 62.5531914893617      [num imgs: 235]\n",
      "3 -> Band Aid: top 1: 20.081967213114755  -  top 5: 37.704918032786885      [num imgs: 244]\n",
      "4 -> Basket: top 1: 5.524861878453039  -  top 5: 23.756906077348066      [num imgs: 181]\n",
      "5 -> Bath towel: top 1: 10.06711409395973  -  top 5: 26.845637583892618      [num imgs: 149]\n",
      "6 -> Beer bottle: top 1: 8.67579908675799  -  top 5: 17.80821917808219      [num imgs: 219]\n",
      "7 -> Bench: top 1: 0.0  -  top 5: 3.0303030303030303      [num imgs: 132]\n",
      "8 -> Bicycle: top 1: 20.408163265306122  -  top 5: 40.816326530612244      [num imgs: 147]\n",
      "9 -> Binder (closed): top 1: 8.241758241758241  -  top 5: 25.274725274725274      [num imgs: 182]\n",
      "10 -> Bottle cap: top 1: 8.870967741935484  -  top 5: 19.35483870967742      [num imgs: 248]\n",
      "11 -> Bread loaf: top 1: 2.262443438914027  -  top 5: 6.334841628959276      [num imgs: 221]\n",
      "12 -> Broom: top 1: 14.655172413793103  -  top 5: 35.775862068965516      [num imgs: 232]\n",
      "13 -> Bucket: top 1: 10.18867924528302  -  top 5: 25.660377358490567      [num imgs: 265]\n",
      "14 -> Butcher's knife: top 1: 8.717948717948717  -  top 5: 24.615384615384617      [num imgs: 195]\n",
      "15 -> Can opener: top 1: 9.313725490196079  -  top 5: 27.941176470588236      [num imgs: 204]\n",
      "16 -> Candle: top 1: 1.7167381974248928  -  top 5: 6.866952789699571      [num imgs: 233]\n",
      "18 -> Chair: top 1: 18.636363636363637  -  top 5: 41.81818181818182      [num imgs: 220]\n",
      "20 -> Coffee/French press: top 1: 2.803738317757009  -  top 5: 14.018691588785046      [num imgs: 107]\n",
      "21 -> Combination lock: top 1: 24.305555555555557  -  top 5: 37.5      [num imgs: 144]\n",
      "22 -> Computer mouse: top 1: 29.017857142857142  -  top 5: 49.107142857142854      [num imgs: 224]\n",
      "23 -> Desk lamp: top 1: 3.5  -  top 5: 15.0      [num imgs: 200]\n",
      "24 -> Dishrag or hand towel: top 1: 0.0  -  top 5: 33.333333333333336      [num imgs: 3]\n",
      "68 -> Printer: top 1: 22.4  -  top 5: 49.6      [num imgs: 125]\n",
      "69 -> Remote control: top 1: 33.333333333333336  -  top 5: 49.18032786885246      [num imgs: 183]\n",
      "70 -> Ruler: top 1: 23.026315789473685  -  top 5: 40.78947368421053      [num imgs: 152]\n"
     ]
    }
   ],
   "source": [
    "# category = 'Banana'\n",
    "\n",
    "res_top1 = []\n",
    "res_top5 = []\n",
    "\n",
    "for n, category in enumerate(data):\n",
    "#     if n > 1:\n",
    "#         break\n",
    "    txtfile = '../' + dirMap[category] + '.txt'\n",
    "    if not os.path.exists(txtfile):\n",
    "        continue\n",
    "    \n",
    "    with open(txtfile) as f:\n",
    "        boxes = [line.strip() for line in f.readlines()]    \n",
    "\n",
    "#     preds = []\n",
    "    count = [0, 0]\n",
    "    lines = 0\n",
    "    for im in boxes[0:]:\n",
    "        ss = im.split(' ')\n",
    "        if len(ss) > 1:\n",
    "            lines += 1\n",
    "            coords = (int(i) for i in ss[1:] if i)\n",
    "            idx = predict_image(os.path.join('../images/' + dirMap[category] + '/', ss[0]), coords, False)\n",
    "            \n",
    "            # top 1\n",
    "            count[0] += 1 if labels[idx[0]] in data[category] else 0  \n",
    "            \n",
    "            # top 5\n",
    "            flag = False\n",
    "            for i in idx[0:]:\n",
    "    #             print(labels[i], end = ' ')\n",
    "#                 predLabels = labels[idxMap[str(i.tolist())]]\n",
    "#                 print(i, labels[i])\n",
    "                  flag = flag or (labels[i] in data[category])\n",
    "            count[1] += 1 if flag else 0\n",
    "#             print('\\n')\n",
    "#             preds.append(idx)\n",
    "\n",
    "\n",
    "\n",
    "    accs = np.array(count)*100/lines\n",
    "    \n",
    "    print(f\"{n} -> {category}: top 1: {accs[0]}  -  top 5: {accs[1]}      [num imgs: {lines}]\")  \n",
    "\n",
    "    res_top1.append(accs[0])        \n",
    "    res_top5.append(accs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.996669092969494\n",
      "30.32472008287954\n"
     ]
    }
   ],
   "source": [
    "print(sum(res_top1)/len(res_top1))\n",
    "print(sum(res_top5)/len(res_top5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = alexnet\n",
    "# for l in preds:\n",
    "#     print(labels[l])\n",
    "# preds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "if ss is ['']:\n",
    "    print('ss')\n",
    "len(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pillow\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.open(\"../images/air_freshener/01d44a4b77b44a0.png\").convert('RGB')\n",
    "\n",
    "img = img.crop((xl, yl, xr, yr))   #((left, top, right, bottom)) \n",
    "img = img.resize((224, 224))\n",
    "\n",
    "\n",
    "img_t = transform(img)#.float()\n",
    "\n",
    "img_t = img_t.permute((2,1,0))\n",
    "\n",
    "img_t = torch.transpose(img_t, 0, 1)\n",
    "\n",
    "# np.clip\n",
    "# img_t = torch.unsqueeze(img_t, 0)\n",
    "# help(img_t.transpose)\n",
    "\n",
    "plt.imshow(img_t)\n",
    "\n",
    "# img = cv2.imread(\"../images/air_freshener/01d44a4b77b44a0.png\", cv2.IMREAD_UNCHANGED)\n",
    "# img = cv2cvLoadImage(\"../images/air_freshener/01d44a4b77b44a0.png\", CV_LOAD_IMAGE_UNCHANGED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(img_t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t[:10,:10].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show(img)\n",
    "%matplotlib inline\n",
    "img = np.array(img)\n",
    "# img = img[:,:,::-1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../air_freshener.txt') as f:\n",
    "    boxes = [line.strip() for line in f.readlines()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl, xr, yr = [int(i) for i in boxes[11].split(' ')[1:] if i]\n",
    "img[yl:yr,xl:xr, :]\n",
    "\n",
    "# std = np.array([0.229, 0.224, 0.225])\n",
    "# inp = std * inp + mean\n",
    "# inp = np.clip(inp, 0, 1)\n",
    "\n",
    "img_t = transform().float()\n",
    "batch_t = torch.unsqueeze(img, 0)\n",
    "\n",
    "\n",
    "plt.imshow(batch_t)\n",
    "# cv2.imshow('t', img[xl:xr, yl:yr,:])\n",
    "# # help(cv2.imshow())\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Image.crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl, yl, xr, yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet101(pretrained=True)\n",
    "\n",
    "alexnet.eval()\n",
    "img_a = img_t.permute((2,1,0))\n",
    "batch_t = torch.unsqueeze(img_a, 0)\n",
    "out = alexnet(batch_t)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out\n",
    "batch_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(out, 1)\n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "print(labels[index[0]], percentage[index[0]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = torch.sort(out, descending=True)\n",
    "2\n",
    "[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cellphone\n",
      "Clothes hamper\n",
      "Mixing / Salad Bowl\n",
      "Monitor\n",
      "Mug\n",
      "Nail (fastener)\n",
      "Necklace\n",
      "Orange\n",
      "Padlock\n",
      "Paintbrush\n",
      "Paper towel\n",
      "Pen\n",
      "Pill bottle\n",
      "Pillow\n",
      "Plastic bag\n",
      "Plate\n",
      "Soap dispenser\n",
      "Sock\n",
      "Soup Bowl\n",
      "Spatula\n",
      "Speaker\n",
      "Still Camera\n",
      "Strainer\n",
      "Stuffed animal\n",
      "Suit jacket\n",
      "Sunglasses\n",
      "Sweater\n",
      "Swimming trunks\n",
      "T-shirt\n",
      "TV\n",
      "Teapot\n",
      "Wheel\n",
      "Whistle\n",
      "Wine bottle\n",
      "Winter glove\n",
      "Wok\n"
     ]
    }
   ],
   "source": [
    "for n, category in enumerate(data):\n",
    "#     if n > 1:\n",
    "#         break\n",
    "    txtfile = '../' + dirMap[category] + '.txt'\n",
    "    if not os.path.exists(txtfile):\n",
    "        print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFolderMap = {}\n",
    "for u, v in folderMap.items():\n",
    "    newFolderMap[v] = u\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data: \n",
    "    if newFolderMap.get(i,0) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wok'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFolderMap[i]\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alarm_clock'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newFolderMap.get('Alarm clock',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Step stool', 'Pill organizer', 'Mouthwash', 'Honey container', 'Key chain', 'Tanktop', 'Document folder (closed)', 'Tarp', 'Webcam', 'Floss container', 'Figurine or statue', 'Tongs', 'Suitcase', 'Skateboard', 'Teabag', 'Earring', 'Wrench', 'Squeeze bottle', 'Shoelace', 'Flashlight', 'Table knife', 'Hair brush', 'Eraser (white board)', 'Tote bag', 'Trash bag', 'Peeler', 'Extension cable', 'Tweezers', 'Dress pants', 'Nut for a screw', 'Paper plates', 'Notebook', 'Ice', 'Phone (landline)', 'Hairtie', 'Laptop charger', 'Makeup', 'Multitool', 'DVD player', 'Sponge', 'Walking cane', 'Ring', 'Kettle', 'Makeup brush', 'Placemat', 'Notepad', 'Deodorant', 'Playing cards', 'Spray bottle', 'Loofah', 'Napkin', 'Nightstand', 'Standing lamp', 'Plastic cup', 'Walker', 'Throw pillow', 'Tape measure', 'Nail clippers', 'Paint can', 'Oven mitts', 'Rock', 'Usb flash drive', 'Poster', 'Toothpaste', 'Video Camera', 'Rolling pin', 'Slipper', 'Flour container', 'Pepper shaker', 'Soap bar', 'Drinking straw', 'Tablecloth', 'Jam', 'Dress shoe (women)', 'Thermometer', 'Trophy', 'Nail file', 'Egg carton', 'Ironing board', 'Tissue', 'Dress shirt', 'Sewing kit', 'Removable blade', 'Drawer (open)', 'Fork', 'Scarf', 'Whisk', 'Ziploc bag', 'Mouse pad', 'Paper bag', 'Photograph (printed)', 'Razor', 'Night light', 'Marker', 'Shampoo bottle', 'Dish soap', 'Receipt', 'Lettuce', 'Dust pan', 'Drying rack for clothes', 'Raincoat', 'Toothbrush', 'Usb cable', 'Shorts', 'Tape / duct tape', 'Stopper (sink/tub)', 'Ice cube tray', 'Egg', 'Leggings', 'Travel case', 'Scrub brush', 'Tablet / iPad', 'Milk', 'Stapler', 'Thermos', 'Handbag', 'Spoon', 'Light bulb', 'Rake', 'Dog bed', 'Nail polish', 'Squeegee', 'Plastic wrap', 'Earbuds', 'Power cable', 'First aid kit', 'Wine glass', 'Ribbon', 'Water filter', 'Hand mirror', 'Newspaper', 'Toy', 'Tomato', 'Power bar', 'Headphones (over ear)', 'Scissors', 'Glue container', 'Magazine', 'Pencil', 'Eyeglasses', 'Sugar container']\n"
     ]
    }
   ],
   "source": [
    "# len(set(newFolderMap.keys()) - set(data))\n",
    "to_do = []\n",
    "for j in set(newFolderMap.keys()) - set(data):\n",
    "    txtfile = '../' + dirMap[j] + '.txt'\n",
    "    if not os.path.exists(txtfile):\n",
    "        to_do.append(j)\n",
    "        \n",
    "print(to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "len(to_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8869204367171575"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8187307530779818"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
